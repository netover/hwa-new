# Resync: AI-Powered HWA/TWS Interface

## Prop√≥sito Principal

O Resync √© uma interface de conversa√ß√£o inteligente, alimentada por Intelig√™ncia Artificial, para comunica√ß√£o com o scheduler de cargas de trabalho **HCL Workload Automation (HWA)**, anteriormente conhecido como IBM Tivoli Workload Scheduler (TWS).

O objetivo √© substituir a complexa navega√ß√£o em pain√©is por uma intera√ß√£o simples e em linguagem natural, centralizando a informa√ß√£o e acelerando a resolu√ß√£o de problemas.

## Como Funciona

O cora√ß√£o do programa √© a integra√ß√£o direta com o TWS atrav√©s de um agente de IA que segue um fluxo claro:

1.  **Pergunta em Linguagem Natural:** Um operador pergunta ao chatbot, por exemplo: *"Qual o status do job `XYZ`?"* ou *"Mostre os jobs que falharam hoje"*.
2.  **A√ß√£o Dupla da IA:** O sistema executa duas a√ß√µes em paralelo:
    *   **Chamada de API em Tempo Real:** O agente de IA executa uma chamada √† API do HWA/TWS para obter o status atual e real do job solicitado.
    *   **Consulta √† Base de Conhecimento (RAG):** Simultaneamente, ele consulta uma base de conhecimento interna (inicialmente um arquivo Excel, mas projetada para ser extens√≠vel) que cont√©m informa√ß√µes adicionais, como procedimentos para tratamento de erros, contatos dos respons√°veis e documenta√ß√£o relevante para aquele job.
3.  **Resposta Consolidada:** O chatbot retorna uma resposta √∫nica e enriquecida para o operador, combinando o status ao vivo do TWS com as informa√ß√µes contextuais e os procedimentos da base de conhecimento.

## Refer√™ncias Oficiais

*   **P√°gina do Produto:** [HCL Workload Automation](https://www.hcl-software.com/workload-automation)
*   **Documenta√ß√£o Oficial:** [HCL Workload Automation Documentation](https://help.hcltechsw.com/workload-automation/v10.2.3/index.html)

---
<br>

# Resync: O Centro de Comando Inteligente para HCL Workload Automation

**Resync n√£o √© apenas um dashboard. √â uma plataforma de intelig√™ncia operacional projetada para transformar a maneira como voc√™ gerencia, monitora e soluciona problemas em seu ambiente HCL Workload Automation (TWS).**

---

## O Problema: A Complexidade da Opera√ß√£o TWS Tradicional

Gerenciar um ambiente TWS robusto √© uma tarefa complexa que tradicionalmente envolve:

-   **Falta de Visibilidade Centralizada:** Operadores precisam navegar por m√∫ltiplas telas e terminais para obter uma vis√£o completa do estado do sistema.
-   **Diagn√≥stico Lento e Reativo:** Quando um job falha (`ABEND`), inicia-se uma ca√ßa manual por logs, documenta√ß√£o e especialistas, resultando em um alto **MTTR (Mean Time to Resolution)**.
-   **Conhecimento Centralizado:** A expertise para resolver problemas complexos geralmente reside em alguns poucos especialistas, criando gargalos e riscos operacionais.
-   **Tarefas Repetitivas e Manuais:** A equipe gasta um tempo valioso em verifica√ß√µes de status e diagn√≥sticos de rotina, em vez de focar em melhorias estrat√©gicas.

## A Solu√ß√£o: Intelig√™ncia, Velocidade e Automa√ß√£o com Resync

Resync aborda esses desafios de frente, aplicando o que h√° de mais moderno em Intelig√™ncia Artificial e engenharia de software para entregar uma experi√™ncia de gerenciamento sem precedentes.

### ‚ú® Principais Recursos Estrat√©gicos

#### 1. **Dashboard Unificado em Tempo Real**
Tenha uma vis√£o completa e ao vivo do seu ambiente TWS em uma √∫nica tela. Monitore o status do sistema, dos motores e dos jobs mais recentes sem precisar sair da interface. A informa√ß√£o vem at√© voc√™.

#### 2. **Chat com IA Especialista em TWS**
Fa√ßa perguntas em linguagem natural e obtenha respostas instant√¢neas. Em vez de navegar por menus complexos, simplesmente pergunte:
-   *"Qual o status do job `FINAL_BATCH_PAYROLL`?"*
-   *"Mostre-me os jobs que falharam nas √∫ltimas 3 horas."*
-   *"Resuma a sa√∫de do ambiente."*

#### 3. **Diagn√≥stico Autom√°tico com RAG (Retrieval-Augmented Generation)**
Este √© o nosso maior diferencial. Quando um job falha, o Resync n√£o apenas informa o erro. Ele **automaticamente** consulta uma base de conhecimento (agora baseada em **Mem0 AI e Qdrant**) e enriquece a resposta com:
-   **Solu√ß√µes Conhecidas:** Passos de troubleshooting baseados em incidentes passados.
-   **Owner do Processo:** Contato do respons√°vel para uma resolu√ß√£o mais r√°pida.
-   **Procedimentos Relevantes:** Links para a documenta√ß√£o ou procedimentos operacionais padr√£o.

#### 4. **Intelig√™ncia Adaptativa e Aprendizado Cont√≠nuo**
Cada intera√ß√£o com o Resync √© registrada e analisada semanticamente. O sistema aprende com as perguntas dos usu√°rios e os problemas que resolve, criando um **Knowledge Graph** que o torna mais inteligente e preciso com o tempo. O **IA Auditor** monitora e refina essa base de conhecimento, sinalizando ou removendo mem√≥rias de baixa qualidade, e o **Dashboard de Revis√£o Humana** permite a curadoria final.

#### 5. **Arquitetura Robusta, Segura e Extens√≠vel**
Constru√≠do sobre uma base s√≥lida com FastAPI, AGNO e as melhores pr√°ticas de software, o Resync √©:
-   **Seguro:** Opera em modo **read-only** para opera√ß√µes no TWS, garantindo que n√£o haja risco de comandos destrutivos. O `IA Auditor` pode remover mem√≥rias incorretas da base de conhecimento interna, mas n√£o afeta o TWS.
-   **Est√°vel:** Possui um cliente de API otimizado com connection pooling e retries para garantir a comunica√ß√£o com o TWS.
-   **Extens√≠vel:** A arquitetura de agentes permite adicionar novas ferramentas e especialidades de IA com facilidade.

---

## üöÄ Vantagens Competitivas: O Valor do Resync

| Vantagem | Sem Resync (M√©todo Tradicional) | Com Resync (Opera√ß√£o Inteligente) |
| :--- | :--- | :--- |
| **Resolu√ß√£o de Falhas (MTTR)** | Horas ou dias de an√°lise manual. | **Segundos** para diagn√≥stico e sugest√£o de solu√ß√£o. |
| **Visibilidade** | Fragmentada, m√∫ltiplas ferramentas. | **Unificada e em tempo real** em um √∫nico dashboard. |
| **Abordagem** | Reativa (age ap√≥s o problema). | **Proativa** (identifica e contextualiza o problema instantaneamente).|
| **Conhecimento** | Centralizado em especialistas. | **Democratizado** e acess√≠vel a toda a equipe via IA. |
| **Efici√™ncia da Equipe** | Focada em tarefas manuais e repetitivas. | Focada em **melhorias estrat√©gicas e otimiza√ß√£o**. |

---

## üõ† Quick Start (Guia T√©cnico)

### 1. Pr√©-requisitos
- Python 3.13+
- Acesso a um ambiente HCL Workload Automation (ou execute em modo mock).

### 2. Instala√ß√£o
Clone o reposit√≥rio:
'''bash
git clone https://github.com/netover/hwa-new-1.git
cd hwa-new-1
'''

Crie um ambiente virtual e instale as depend√™ncias:
'''bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
'''

### 3. Configura√ß√£o
Crie um arquivo `.env` na raiz do projeto a partir do exemplo `.env.example`. Este arquivo centraliza todas as configura√ß√µes sens√≠veis e de ambiente.

#### Configura√ß√µes Principais

*   `TWS_HOST`, `TWS_PORT`, `TWS_USER`, `TWS_PASSWORD`: Credenciais para a conex√£o com a API do HWA/TWS.
*   `TWS_MOCK_MODE`: Defina como `True` para usar dados de exemplo (`mock_tws_data.json`) sem precisar de uma conex√£o real com o TWS. Ideal para desenvolvimento.
*   `APP_ENV`: Define o perfil da aplica√ß√£o (`development` ou `production`).
*   `LLM_ENDPOINT`, `LLM_API_KEY`, `AGENT_MODEL_NAME`, `AUDITOR_MODEL_NAME`: Configura√ß√µes do provedor de LLM. Veja exemplos detalhados abaixo.

---

#### Exemplos de Configura√ß√£o do LLM

Gra√ßas √† biblioteca `litellm`, voc√™ pode configurar o sistema para usar tanto LLMs rodando localmente quanto modelos acessados via API na nuvem. A escolha √© feita atrav√©s das vari√°veis de ambiente.

##### Exemplo 1: Usando um LLM Local com Ollama

Ideal para desenvolvimento, testes e ambientes que exigem que os dados n√£o saiam da sua rede.

'''env
# Aponta para o endpoint local do Ollama
LLM_ENDPOINT=http://localhost:11434/v1

# Chave de API n√£o √© necess√°ria para Ollama
LLM_API_KEY=

# Nome do modelo que voc√™ baixou com "ollama pull"
AGENT_MODEL_NAME=llama3
AUDITOR_MODEL_NAME=llama3
'''

---

A seguir, exemplos de como usar provedores de nuvem via API.

##### Exemplo 2: Usando a API da OpenAI

Para usar modelos de alta performance como GPT-4o diretamente da OpenAI.

'''env
# O endpoint pode ser omitido, pois a litellm usar√° o padr√£o da OpenAI
LLM_ENDPOINT=

# Sua chave de API da OpenAI
LLM_API_KEY=sk-proj-...

# Especifique os modelos desejados
AGENT_MODEL_NAME=gpt-4o
AUDITOR_MODEL_NAME=gpt-4o-mini
'''

##### Exemplo 3: Usando OpenRouter

Para acessar uma vasta gama de modelos de diferentes provedores (Google, Mistral, Anthropic, etc.) com uma √∫nica API.

'''env
# Aponta para o endpoint da API do OpenRouter
LLM_ENDPOINT=https://openrouter.ai/api/v1

# Sua chave de API do OpenRouter
LLM_API_KEY=sk-or-v1-abc...xyz

# Especifique o modelo desejado, prefixando com "openrouter/"
AGENT_MODEL_NAME=openrouter/google/gemini-pro-1.5
AUDITOR_MODEL_NAME=openrouter/google/gemini-pro-1.5
'''

##### Exemplo 4: Usando a API da NVIDIA NIM

Para usar os modelos otimizados da NVIDIA atrav√©s de sua API. A integra√ß√£o √© simples pois a API da NVIDIA tamb√©m √© compat√≠vel com o padr√£o da OpenAI.

'''env
# 1. Aponte para o endpoint da API da NVIDIA
LLM_ENDPOINT=https://integrate.api.nvidia.com/v1

# 2. Insira sua chave de API da NVIDIA
LLM_API_KEY=nvapi-abc...xyz

# 3. Especifique o ID do modelo da NVIDIA
AGENT_MODEL_NAME=meta/llama3-8b-instruct
AUDITOR_MODEL_NAME=meta/llama3-8b-instruct
'''

---

#### Preced√™ncia e Como For√ßar uma Configura√ß√£o

**A regra √© simples: o nome do modelo (`AGENT_MODEL_NAME`) √© o que manda.**

A biblioteca `litellm` decide para qual provedor enviar a requisi√ß√£o com base no formato do nome do modelo, n√£o em qual vari√°vel de ambiente est√° preenchida.

##### Como For√ßar o Uso de um LLM Local (Ex: Ollama)

Para garantir que o sistema use seu LLM local, defina um **nome de modelo gen√©rico** (sem prefixos) e aponte o `LLM_ENDPOINT` para seu servidor local.

'''env
# 1. Aponte para o endpoint local
LLM_ENDPOINT=http://localhost:11434/v1

# 2. Defina um nome de modelo gen√©rico (ex: 'llama3')
AGENT_MODEL_NAME=llama3

# Mesmo que a chave de API abaixo esteja preenchida, ela ser√° ignorada
# porque o nome do modelo n√£o direciona para um provedor de API.
LLM_API_KEY=sk-or-v1-abc...xyz
'''

##### Como For√ßar o Uso de um LLM Externo via API (Ex: OpenRouter)

Para garantir que o sistema use uma API externa, use o **prefixo espec√≠fico do provedor** no nome do modelo (se aplic√°vel, como no OpenRouter) ou simplesmente configure o `LLM_ENDPOINT` e `LLM_API_KEY` para o provedor desejado. O prefixo no nome do modelo sempre ter√° prioridade sobre o `LLM_ENDPOINT`.

'''env
# 1. Use o prefixo do provedor no nome do modelo
AGENT_MODEL_NAME=openrouter/google/gemini-pro-1.5

# 2. Defina a chave de API correspondente
LLM_API_KEY=sk-or-v1-abc...xyz

# Mesmo que o endpoint local abaixo esteja preenchido, ele ser√° ignorado
# porque o prefixo 'openrouter/' for√ßa o uso da API externa.
LLM_ENDPOINT=http://localhost:11434/v1
'''

### 4. Executando a Aplica√ß√£o
Para iniciar o servidor backend:

**Modo Desenvolvimento (com Mock TWS):**
'''bash
# Certifique-se que TWS_MOCK_MODE=True e APP_ENV=development no seu .env
uvicorn resync.main:app --reload --host 0.0.0.0 --port 8000
'''
A aplica√ß√£o estar√° dispon√≠vel em `http://localhost:8000`.

**Modo Produ√ß√£o (com TWS real):**
'''bash
# Certifique-se que TWS_MOCK_MODE=False e APP_ENV=production no seu .env
# E que as vari√°veis TWS_HOST, TWS_PORT, TWS_USER, TWS_PASSWORD estejam configuradas
uvicorn resync.main:app --host 0.0.0.0 --port 8000
'''

### 5. Acessando as Interfaces
*   **Dashboard Principal:** `http://localhost:8000/dashboard`
*   **Chat com IA:** `http://localhost:8000/` (redireciona para o dashboard)
*   **Revis√£o de Mem√≥rias (IA Auditor):** `http://localhost:8000/revisao`
*   **M√©tricas Prometheus:** `http://localhost:8000/api/metrics`
*   **Documenta√ß√£o da API (Swagger UI):** `http://localhost:8000/docs`

### 6. Executando Testes
Para rodar os testes unit√°rios e a an√°lise est√°tica:
'''bash
# Rodar testes unit√°rios
make test

# Rodar linters e type checker
make fmt
'''

### 7. Docker (Opcional)
O projeto inclui um Dockerfile para containeriza√ß√£o, mas o uso de Docker √© opcional. Voc√™ pode executar a aplica√ß√£o diretamente com Uvicorn conforme descrito na se√ß√£o 4.
